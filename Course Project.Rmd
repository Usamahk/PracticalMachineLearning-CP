---
title: "Practical Machine Learning - Course Project"
author: "Usamah Khan"
date: "August 20, 2015"
output: html_document
---

## Overview

Nowadays it seems like everyone at the gym or the park wears the coloured band of a FitBit, Nike FuelBand, Jawbone Up or some other activity tracker on their arm. You may very well have one on as you read this. These tools aim to collect huge amounts of data about personal activity, relatively inexpensively and easily. This allows us to take measurements of ourselves performing activites such as running, swimming and lifting so as to quantify how much we do to improve our health. However, rarely do we try to **quantify** *how well* we perform an activity.

The Human Activity Recognitoin Project (HAR) aims to do just that. By placing accelerometers on the belt, forearm, arm and dumbell of 6 participants who were all asked to perform lifts correctly and incorrectly (using light weights under proper supervision to avoid injury). The goal of the project was to examine how could they determine a reference for how well an exercise is being performed.

By taking the data and sets of "good" and "bad" workouts we can create tools that can help us stay on track of performing workouts well and safely. The purpose of this project is to build Machine Learning algorithm using different packages in R to determine these questions. R has many ML libraries available and as such determining the pros and cons of each is a useful undertaking.

## Loading libraries

So now that the purpose of this project has been outlined as to test packages and contrast the differences between them, as part of showing my workflow, I've loaded up my packages first and set a random seed for reproducibility if anyone wants to have a go. 

```{r Loading Packages, cache=TRUE}
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(e1071)

set.seed(12321)
```

## Data

### Reading

All information surrounding and describing the experiment can be found here: http://groupware.les.inf.puc-rio.br/har. The data can be found at the following Urls and downloaded as shown. 

```{r Downloading the data, cache=TRUE}
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(fileUrl1, destfile = "./train.csv", method = "curl")
download.file(fileUrl2, destfile = "./test.csv", method = "curl")

train_set <- read.csv("train.csv", header = TRUE, sep = ",", na.strings=c("",NA))
test_set <- read.csv("test.csv", header = TRUE, sep = ",", na.strings=c("",NA))

dim(train_set)
dim(test_set)
```

### Pre-processing

The first step in building any Machine Learning problem is to pre-process and clean the data.

As far as we can tell the data is clean and has the 160 variables we need and the test set is comprised of the full 20 variables. Looking through however, in the data we can observe many cases of missing values and *NAs*. This poses an issue since model fitting with missing values usually results in error.

To deal with this we could either impute the data or find ways to remove the data. Imputation has its own complications. In this case we can see that since the missing values occur systematically, the best course of action seems to be to remove the columns where a certain threshold of missing values is exceeded. In this case, lets say 90%.

```{r Preprocessing the data, cache=TRUE}
df <- data.frame(1)
df2 <- data.frame(1)
count = 1

for(i in 1:ncol(train_set)){
    df[i,] <- (sum(is.na(train_set[,i]))/19622)
      if (df[i,] > 0.9) {
        df2[count,] = i 
        count = count + 1
      }
}

count = 0

for(i in 1:nrow(df2)){
  train_set[,(df2[i,]-count)] <- NULL
  test_set[,(df2[i,]-count)] <- NULL
  count = count + 1
  }
```

Finally, since the first 7 columns seem to be for bookkeeping processes only, they too can be dropped.

```{r Dropping the final values, cache=TRUE}
train_set <- train_set[,-c(1,2,3,4,5,6,7)]
test_set <- test_set[,-c(1,2,3,4,5,6,7)]
```

Voila. Now, with the data fully pre-processed, we can begin to fit and play with some models.

## Building Models

So now that we're trying to determine an effective model, we can't very well use our *test* set to build upon. So, to move forward on this we have to partition our *training* data set. Any reasonable partition can be used, I generally like to stick to a 60/40 split.

```{r Partitioning the Data, cache=TRUE}
inTrain <- createDataPartition(train_set$classe, p = 0.6, list = FALSE)
train_model <- train_set[inTrain,]
train_validate <- train_set[-inTrain,]

dim(train_model)    
dim(train_validate) 
```

We can now begin using the data sets to create our prediction models. To test the packages on effectiveness and efficiency we can set it up to call upon the system time functions to determine how long the models take to train.

### Model 1: Decision Trees

Let's start building the first model designed with the use of *Decision Trees* for prediction. Decision trees map information and obeservations of variables in a data set to conclusions about the target value, i.e. what classification. It is one of the simpler, more basic forms of Machine Learning however it's also an extremely useful *visual* tool. *Leaves* are used to show classification while *branches* display a conjunction of features and their strength in the model leading to those conclusions. 

A simple tree can be built with the use of the *caret* package calling upon the *rpart* method in *train*.

```{r Using Trees, cache=TRUE}
startTime <- Sys.time()

modFit <- train(classe ~ ., data = train_model, method = "rpart")

runTime <- Sys.time() - startTime
runTime

pred_1 <- predict(modFit, train_validate)
confusionMatrix(pred_1, train_validate$classe)
```

We can also call upon the rattle package to create the plot of the tree to see the flow.

```{r Print Trees, cache=TRUE}
fancyRpartPlot(modFit$finalModel)
```

The results of the confusion matrix aren't promising and the statistics show this model as very weak with only a *48%* accuracy, with a run time of *38.7 seconds*. This was to be expected since with the amount of variables, it would be hard to accurately fit a tree model. This tool comes from the *caret* package so we can ask, would another package, attempting the same thing, be as effective? 

### Model 2: Decision Trees

Another package that can help with decision trees is the standalone *Rpart* package. To call and use the tools, it works very similarly to *train* from the *caret* package.

```{r Using Rpart, cache=TRUE}
startTime <- Sys.time()

modFit_rp <- rpart(classe ~ ., data = train_model, method = "class")

runTime <- Sys.time() - startTime
runTime

pred_2 <- predict(modFit_rp, train_validate, type = "class")
confusionMatrix(pred_2, train_validate$classe)
```

```{r Plotting with Fancy Rpart Plot, cache=TRUE}
fancyRpartPlot(modFit_rp)
```

Immmediately, we can see that the *Rpart* package seems to be much more effective than the tools included in the *caret* package. The decision tree plot works to classify more effectively and the confusion matrix and statistics show us a *72.6%* accuracy and a runtime of *2.05 seconds* - **extremely** fast. 

It would seem that we could stop here as a *72.6%* accuracy, while in need of improvement, can definitely be considered a *good* result. This is quite a clean dataset and as such the perfomance seems a little inflated for a decision tree. Trees don't have optimal performance when compared to other methods, especially when dealing with other variables so to further enhance this accuracy we can employ the use of other tools such as *bagging*, *boosting* or *Random Forests*.

### Model 3: Random Forests

The next model we can build with *Random Forests* to see if the accuracy can be improved. Random Forests take the decision tree format further by employing the use of a large number of decision trees. Small portions of the data are randomly selected and then key sets of features are decided upon to grow each decision tree. These all give different error rates and the collection of trees (forest) are compared to find a set of variables that are strongest in prediction.

There exists an R package that we can use to create this model; ***randomForest***.

```{r Using Random Forests, cache = TRUE}
startTime <- Sys.time()

modFit_rf <- randomForest(classe ~ ., data = train_model)

runTime <- Sys.time() - startTime
runTime

pred_3 <- predict(modFit_rf, train_validate)
confusionMatrix(pred_3, train_validate$classe)
```

We can make a quick plot of the model using some tools from the *Rpart.plot* package.

```{r Plotting trees, cache=TRUE}
treeModel <- rpart(classe ~ ., data=train_model, method="class")
prp(treeModel) 
```

Now, this is a much superior model with a level of accuracy, when applied to the test set, of *99.2%*. The runtime however seems to have slowed down at *30.06 seconds*.

## Prediction using Final Model

At this point we can assume the Random Forest model to be the most accurate to apply to the test set. To further enhance the accuracy (although not necessary) we can train the model to the entire training set and then apply to the test set.

```{r Applying over the entire set, cache=TRUE}
startTime <- Sys.time()

Final_modFit_rf <- randomForest(classe ~ ., data = train_set)

runTime <- Sys.time() - startTime
runTime
```

As we apply the training over the whole training set the run time is subsequently longer clocking in at *56.9 seconds*. With this complete the last step is to predict on the test set using the Model.

*NOTE - this chunk of code can be used to get an output of the results to check against the correct answers of the test set*
```{r Prediciton and printing of answers, cache=TRUE}
final_pred <- predict(Final_modFit_rf, test_set)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(final_pred)
```

By checking our answers, we can find a result of ***20/20***. Pretty good work.

## Conclusion

The objective of the project was to examine the data set and apply machine learning techniques to accurately and quantitatively predict how well an exercise is being performed. The data was already split into a training and test set and the training set was used to create a model that could then be applied to the test set to predict the exercise in question. This dataset was quite clean and accurate and we should note that rarely will one with such little pre-processing, high accuracy and little model tuning be available. However, for the purposes of this project, we used this to our advantage to achieve a (more than) satisfactory result.

The three packages all seemed tp have their pros and cons. The *caret* package offers a simple solution yet one with less than an effective result for classification and regression routines. 

Using *Rpart* gave us a satisfactory result, one that could have been better, but one with a lightning quick *2.05 seconds* to run. In terms of practicality, the tool seems great used in conjuction with pre-processing of data. At each step of of pre-pro - cutting variables, cleaning them or imputing for missing values - one could easily run a quick model and compare its accuracy relative to each previous iteration. This in turn would be a great indicator of how well a pre-processing strategy is turning out. 

However, once a good and clean set of variables are determined, for optimal accuracy, using *Random Forests* would be best since they are miles ahead in effectiveness, while not as speedy.

These are basic Machine Learning techniques. Much more powerful methods, such as Neural Networks, exist and can tackle more complex problems. For classification with small-mid size datasets however, the methods explored above can be perfectly effective.

## Notes and References

The project was begun as part of Coursework for the Coursera Practical Machine Learning Course as part of the Data Science Specialization offered by John's Hopkins Unversity. It was taken a step further from the original outline with additional packages being explored and contrasted. The data and project overview was provided by http://groupware.les.inf.puc-rio.br/har under a creative commons license for the purposes of learning. Feel free to get in touch if there are errors, questions or concerns you may have regarding my approach or use of the data. I'd be happy to chat about and learn from it!

*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H.* **Qualitative Activity Recognition of Weight Lifting Exercises** *. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.*

*http://www.statmethods.net/advstats/cart.html*

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4E7hwtjlb